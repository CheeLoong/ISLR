---
title: "ISLR - C3Applied - Q15: Inferences from SLR vs MLR"
author: "Chee Loong Lian"
date: "12/18/2017"
output: html_document
---

**Q15(a) This problem involves the "Boston" data set, per capita crime rate is the response, and the other variables are the predictors. For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response ? Create some plots to back up your assertions.**

```{r}
library(MASS)
attach(Boston)
fit.zn <- lm(crim ~ zn)
summary(fit.zn)
fit.indus <- lm(crim ~ indus)
summary(fit.indus)
chas <- as.factor(chas)
fit.chas <- lm(crim ~ chas)
summary(fit.chas)
fit.nox <- lm(crim ~ nox)
summary(fit.nox)
fit.rm <- lm(crim ~ rm)
summary(fit.rm)
fit.age <- lm(crim ~ age)
summary(fit.age)
fit.dis <- lm(crim ~ dis)
summary(fit.dis)
fit.rad <- lm(crim ~ rad)
summary(fit.rad)
fit.tax <- lm(crim ~ tax)
summary(fit.tax)
fit.ptratio <- lm(crim ~ ptratio)
summary(fit.ptratio)
fit.black <- lm(crim ~ black)
summary(fit.black)
fit.lstat <- lm(crim ~ lstat)
summary(fit.lstat)
fit.medv <- lm(crim ~ medv)
summary(fit.medv)
```

To find which predictors are significant, we have to test $H_0 : \beta_1 = 0$. All predictors have a p-value less than 0.05 except "chas", so we may conclude that there is a statistically significant association between each predictor and the response except for the "chas" predictor in this particular case.

**(b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis $H_0 : \beta_j = 0$ ?**

```{r}
fit.all <- lm(crim ~ ., data = Boston)
summary(fit.all)
```

We may reject the null hypothesis for "zn", "dis", "rad", "black" and "medv".

**(c) How do your results from (a) compare to your results from (b) ? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point on the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.**

```{r}
simple.reg <- vector("numeric",0)
simple.reg <- c(simple.reg, fit.zn$coefficient[2])
simple.reg <- c(simple.reg, fit.indus$coefficient[2])
simple.reg <- c(simple.reg, fit.chas$coefficient[2])
simple.reg <- c(simple.reg, fit.nox$coefficient[2])
simple.reg <- c(simple.reg, fit.rm$coefficient[2])
simple.reg <- c(simple.reg, fit.age$coefficient[2])
simple.reg <- c(simple.reg, fit.dis$coefficient[2])
simple.reg <- c(simple.reg, fit.rad$coefficient[2])
simple.reg <- c(simple.reg, fit.tax$coefficient[2])
simple.reg <- c(simple.reg, fit.ptratio$coefficient[2])
simple.reg <- c(simple.reg, fit.black$coefficient[2])
simple.reg <- c(simple.reg, fit.lstat$coefficient[2])
simple.reg <- c(simple.reg, fit.medv$coefficient[2])
mult.reg <- vector("numeric", 0)
mult.reg <- c(mult.reg, fit.all$coefficients)
mult.reg <- mult.reg[-1]
plot(simple.reg, mult.reg, col = "red")
```

Coefficient for nox is approximately -10 in univariate model and 31 in multiple regression model.

There is a difference between the simple and multiple regression coefficients. This difference is due to the fact that in the simple regression case, the slope term represents the average effect of an increase in the predictor, ignoring other predictors. In contrast, in the multiple regression case, the slope term represents the average effect of an increase in the predictor, while holding other predictors fixed. It does make sense for the multiple regression to suggest no relationship between the response and some of the predictors while the simple linear regression implies the opposite because the correlation between the predictors show some strong relationships between some of the predictors.

```{r}
cor(Boston[-c(1, 4)])
```

So for example, when "age" is high there is a tendency in "dis" to be low, hence in simple linear regression which only examines "crim" versus "age", we observe that higher values of "age" are associated with higher values of "crim", even though "age" does not actually affect "crim". So "age" is a surrogate for "dis"; "age" gets credit for the effect of "dis" on "crim".

**(d) Is there evidence of non-linear association between any of the predictors and the response ? To answer this question, for each predictor $X$, fit a model of the form**
\[Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \varepsilon.\]

```{r}
fit.zn2 <- lm(crim ~ poly(zn, 3))
summary(fit.zn2)
fit.indus2 <- lm(crim ~ poly(indus, 3))
summary(fit.indus2)
fit.nox2 <- lm(crim ~ poly(nox, 3))
summary(fit.nox2)
fit.rm2 <- lm(crim ~ poly(rm, 3))
summary(fit.rm2)
fit.age2 <- lm(crim ~ poly(age, 3))
summary(fit.age2)
fit.dis2 <- lm(crim ~ poly(dis, 3))
summary(fit.dis2)
fit.rad2 <- lm(crim ~ poly(rad, 3))
summary(fit.rad2)
fit.tax2 <- lm(crim ~ poly(tax, 3))
summary(fit.tax2)
fit.ptratio2 <- lm(crim ~ poly(ptratio, 3))
summary(fit.ptratio2)
fit.black2 <- lm(crim ~ poly(black, 3))
summary(fit.black2)
fit.lstat2 <- lm(crim ~ poly(lstat, 3))
summary(fit.lstat2)
fit.medv2 <- lm(crim ~ poly(medv, 3))
summary(fit.medv2)
```

For "zn", "rm", "rad", "tax" and "lstat" as predictor, the p-values suggest that the cubic coefficient is not statistically significant; for "indus", "nox", "age", "dis", "ptratio" and "medv" as predictor, the p-values suggest the adequacy of the cubic fit; for "black" as predictor, the p-values suggest that the quandratic and cubic coefficients are not statistically significant, so in this latter case no non-linear effect is visible.