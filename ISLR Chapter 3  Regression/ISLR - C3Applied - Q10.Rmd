---
title: 'ISLR - Chapter 3 Linear Regression: Applied Q10: More on MLR'
author: "Chee Loong Lian"
date: "12/13/2017"
output: html_document
---

**Q10(a) Fit MLR on the "Carseats" data set to predict "Sales" Using "Price", "Urban", "US"**
```{r}
library(ISLR)
attach(Carseats)
lm.fit = lm(Sales ~ Price + Urban + US)
summary(lm.fit)
```

**(b) Provide an interpretation of each coefficient in the model.**

* "Price" variable: The average effect of a price increase of 1 dollar is a decrease of `r abs(summary(lm.fit)$coef[2, 1]) * 1000` units in sales all other predictors remaining fixed.
* "Urban" variable: On average the unit sales in urban location are `r abs(summary(lm.fit)$coef[3, 1]) * 1000` units less than in rural location all other predictors remaining fixed.
* "US" variable: On average the unit sales in a US store are `r abs(summary(lm.fit)$coef[4, 1]) * 1000` units more than in a non US store all other predictors remaining fixed.

**(c) Write out the model in equation form, being careful to handle the qualitative variables properly.**

The model may be written as \[Sales = `r summary(lm.fit)$coef[1, 1]` + (`r summary(lm.fit)$coef[2, 1]`)\times Price + (`r summary(lm.fit)$coef[3, 1]`)\times Urban + (`r summary(lm.fit)$coef[4, 1]`)\times US + \varepsilon\] with $Urban = 1$ if the store is in an urban location and $0$ if not, and $US = 1$ if the store is in the US and $0$ if not.

**(d)For which of the predictors can you reject the null hypothesis $H_0 : \beta_j = 0$ ?**

We can reject the null hypothesis for the "Price" and "US" variables.

**(e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.**
```{r}
lm.fit2 = lm(Sales ~ Price + US)
summary(lm.fit2)
```

**(f) How well do the models in (a) and (e) fit the data ?**

Based on the RSE and $R^2$ of the linear regressions, they both fit the data similarly, with linear regression from (e) fitting the data slightly better. Essentially about `r summary(lm.fit2)$r.sq * 100`% of the variability is explained by the second model.

**(g) Using the model from (e), obtain 95% confidence intervals for the coefficient(s).**
```{r}
confint(lm.fit2)
```

**(h) Is there evidence of outliers or high leverage observations in the model from (e) ?**
```{r}
plot(predict(lm.fit2), rstudent(lm.fit2))
```

All studentized residuals appear to be bounded by (-3 to 3), so not potential outliers are suggested from the linear regression.

```{r}
par(mfrow = c(2,2))
plot(lm.fit2)
```

However, there are some points that exceed $(p + 1)/n$ (`r (2 + 1) / 400`) that suggest that the corresponding points have high leverage.






















