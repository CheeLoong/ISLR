---
title: "ISLR - C3Applied - Q11,Q12: Regression Without Intercept?"
author: "Chee Loong Lian"
date: "12/18/2017"
output: html_document
---

**Q11. In this problem we will investigate the t-statistic for the null hypothesis $H_0 : \beta = 0$ in simple linear regression without an intercept. To begin, we generate a predictor $x$ and a response $y$ as follows.**
```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
```


**(a) Perform a simple linear regression of $y$ onto $x$, without an intercept. Report the coefficient estimate $\hat{\beta}$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0$. Comment on these results.**
```{r}
lm.fit <- lm(y ~ x + 0)
summary(lm.fit)
```

According to the summary above, we have a value of `r summary(lm.fit)$coef[1, 1]` for $\hat{\beta}$, a value of `r summary(lm.fit)$coef[1, 2]` for the standard error, a value of `r summary(lm.fit)$coef[1, 3]` for the t-statistic and a value of `r summary(lm.fit)$coef[1, 4]` for the p-value. The small p-value allows us to reject $H_0$.

**(b) Now perform a simple linear regression of $x$ onto $y$, without an intercept. Report the coefficient estimate $\hat{\beta}$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis $H_0$. Comment on these results.**
```{r}
lm.fit2 <- lm(x ~ y + 0)
summary(lm.fit2)
```

According to the summary above, we have a value of `r summary(lm.fit2)$coef[1, 1]` for $\hat{\beta}$, a value of `r summary(lm.fit2)$coef[1, 2]` for the standard error, a value of `r summary(lm.fit2)$coef[1, 3]` for the t-statistic and a value of `r summary(lm.fit2)$coef[1, 4]` for the p-value. The small p-value allows us to reject $H_0$.

**(c) What is the relationship between the results obtained in (a) and (b) ?**

We obtain the same value for the t-statistic and consequently the same value for the corresponding p-value. Both results in (a) and (b) reflect the same line created in (a). In other words, $y = 2x + \varepsilon$ could also be written $x = 0.5(y âˆ’ \varepsilon)$.

**(d) For the regrssion of $Y$ onto $X$ without an intercept, the t-statistic for $H_0 : \beta = 0$ takes the form $\hat{\beta}/SE(\hat{\beta})$, where $\hat{\beta}$ is given by (3.38), and where
\[SE(\hat{\beta}) = \sqrt{\frac{\sum_{i=1}^n(y_i - x_i\hat{\beta})^2}{(n - 1)\sum_{i=1}^nx_i^2}}.\]
Show algebraically, and confirm numerically in R, that the t-statistic can be written as
\[\frac{\sqrt{n - 1}\sum_{i=1}^nx_iy_i}{\sqrt{(\sum_{i=1}^nx_i^2)(\sum_{i=1}^ny_i^2) - (\sum_{i=1}^nx_iy_i)}}.\]

We have
$$
\begin{array}{cc}
t = \beta / SE(\beta) &
\beta = \frac {\sum{x_i y_i}} {\sum{x_i^2}} &
SE(\beta) = \sqrt{\frac {\sum{(y_i - x_i \beta)^2}} {(n-1) \sum{x_i^2}}}
\end{array}
\\
t = {\frac {\sum{x_i y_i}} {\sum{x_i^2}}}
    {\sqrt{\frac {(n-1) \sum{x_i^2}} {\sum{(y_i - x_i \beta)^2}}}}
\\
\frac {\sqrt{n-1} \sum{x_i y_i}}
      {\sqrt{\sum{x_i^2} \sum{(y_i - x_i \beta)^2}}}
\\
\frac {\sqrt{n-1} \sum{x_i y_i}}
      {\sqrt{\sum{x_i^2} \sum{(y_i^2 - 2 \beta x_i y_i  + x_i^2 \beta^2)}}}
\\
\frac {\sqrt{n-1} \sum{x_i y_i}}
      {\sqrt{\sum{x_i^2} \sum{y_i^2} - 
            \sum{x_i^2} \beta (2 \sum{x_i y_i} - \beta \sum{x_i^2})}}
\\
\frac {\sqrt{n-1} \sum{x_i y_i}}
      {\sqrt{\sum{x_i^2} \sum{y_i^2} - 
            \sum{x_i y_i} (2 \sum{x_i y_i} - \sum{x_i y_i})}}
\\
t = \frac {\sqrt{n-1} \sum{x_i y_i}} 
          {\sqrt{\sum{x_i^2} \sum{y_i^2} - (\sum{x_i y_i})^2 }}
$$

Now let's verify this result numerically.
```{r}
n <- length(x)
t <- sqrt(n - 1)*(x %*% y)/sqrt(sum(x^2) * sum(y^2) - (x %*% y)^2)
as.numeric(t)
```

We may see that the t above is exactly the t-statistic given in the summary of lm.fit2.

**(e) Using the results from (d), argue that the t-statistic for the regression of $y$ onto $x$ is the same t-statistic for the regression of $x$ onto $y$.**

It is easy to see that if we replace $x_i$ by $y_i$ in the formula for the t-statistic, the result would be the same.

**(f) In R, show that when regression is performed with an intercept, the t-statistic for $H_0 : \beta_1 = 0$ is the same for the regression of $y$ onto $x$ as it is the regression of $x$ onto $y$.**

```{r}
lm.fit = lm(y~x)
lm.fit2 = lm(x~y)
summary(lm.fit)
summary(lm.fit2)
```

It is again easy to see that the t-statistic for "lm.fit" and "lm.fit2" are both equal to `r summary(lm.fit)$coef[2, 3]`.

**Q12(a) Recall that the coefficient estimate $\hat{\beta}$ for the linear regression of $Y$ onto $X$ without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of $X$ onto $Y$ the same as the coefficient estimate for the regression of $Y$ onto $X$ ?**

The coefficient estimate for the regression of $Y$ onto $X$ is
\[\hat{\beta} = \frac{\sum_ix_iy_i}{\sum_jx_j^2};\]
The coefficient estimate for the regression of $X$ onto $Y$ is
\[\hat{\beta}' = \frac{\sum_ix_iy_i}{\sum_jy_j^2}.\]
The coefficients are the same iff $\sum_jx_j^2 = \sum_jy_j^2$.

**(b) Generate an example in R with $n = 100$ observations in which the coefficient estimate for the regression of $X$ onto $Y$ is different from the coefficient estimate for the regression of $Y$ onto $X$.**
```{r}
set.seed(1)
x <- 1:100
sum(x^2)
y <- 2 * x + rnorm(100, sd = 0.1)
sum(y^2)
fit.Y <- lm(y ~ x + 0)
fit.X <- lm(x ~ y + 0)
summary(fit.Y)
summary(fit.X)
```

**(c) Generate an example in R with $n = 100$ observations in which the coefficient estimate for the regression of $X$ onto $Y$ is the same as the coefficient estimate for the regression of $Y$ onto $X$.**
```{r}
x <- 1:100
sum(x^2)
y <- 100:1
sum(y^2)
fit.Y <- lm(y ~ x + 0)
fit.X <- lm(x ~ y + 0)
summary(fit.Y)
summary(fit.X)
```




