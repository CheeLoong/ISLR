---
title: "ISLR - C3Applied - Q14: Collinearity"
author: "Chee Loong Lian"
date: "12/18/2017"
output: html_document
---

**Q14(a) This problem focuses on the collinearity problem. Perform the following commands in R.**

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

**The last line corresponds to creating a linear model in which "y" is a function of "x1" and "x2". Write out the form of the linear model. What are the regression coefficients ?**

*The form of the linear model is
\[Y = 2 + 2X_1 +0.3X_2 + \varepsilon\]
with $\varepsilon$ ~ $N(0,1)$ random variable. The regression coefficients are respectively 2, 2 and 0.3.*

**(b) What is the correlation between "x1" and "x2" ? Create a scatterplot displaying the relationship between the variables.**

```{r}
cor(x1, x2)
plot(x1, x2)
```

The variables seem highly correlated.

**(c) Using this data, fit a least squares regression to predict "y" using "x1" and "x2". Describe the results obtained. What are $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\beta}_2$ ? How do these relate to the true $\beta_0$, $\beta_1$ and $\beta_2$ ? Can you reject the null hypothesis $H_0 : \beta_1 = 0$ ? How about the null hypothesis $H_0 : \beta_2 = 0$ ?**

```{r}
lm.fit <- lm(y ~ x1 + x2)
summary(lm.fit)
```

The coefficients $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\beta}_2$ are respectively `r summary(lm.fit)$coef[1, 1]`, `r summary(lm.fit)$coef[2, 1]` and `r summary(lm.fit)$coef[3, 1]`. Only $\hat{\beta}_0$ is close to $\beta_0$. As the p-value is less than 0.05 we may reject $H_0$ for $\beta_1$, however we may not reject $H_0$ for $\beta_2$ as the p-value is higher than 0.05.

**(d) Now fit a least squares regression to predict "y" using only "x1". Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$ ?**

```{r}
lm.fit2 <- lm(y ~ x1)
summary(lm.fit2)
```

The coefficient for "x1" in this last model is very different from the one with "x1" and "x2" as predictors. In this case "x1" is highly significant as its p-value is very low, so we may reject $H_0$.

**(e) Now fit a least squares regression to predict "y" using only "x2". Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$ ?**

```{r}
lm.fit3 <- lm(y ~ x2)
summary(lm.fit3)
```

The coefficient for "x2" in this last model is very different from the one with "x1" and "x2" as predictors. In this case "x2" is highly significant as its p-value is very low, so we may again reject $H_0$.

**(f) Do the results obtained in (c)-(e) contradict each other ? Explain your answer.**

No, the results do not contradict each other. As the predictors "x1" and "x2" are highly correlated we are in the presence of collinearity, in this case it can be difficult to determine how each predictor separately is associated with the response. Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\hat{\beta}_1$ to grow (we have a standard error of `r summary(lm.fit)$coef[2, 2]` and `r summary(lm.fit)$coef[3, 2]` for "x1" and "x2" respectively in the model with two predictors and only of `r summary(lm.fit2)$coef[2, 2]` and `r summary(lm.fit3)$coef[2, 2]` for "x1" and "x2" respectively in the models with only one predictor). Consequently, we may fail to reject $H_0$ in the presence of collinearity. The importance of the "x2" variable has been masked due to the presence of collinearity.

**(g) Now suppose we obtain one additional observation, which was unfortunately mismeasured.**

```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```

**Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on each of the models ? In each model, is this observation an outlier ? A high-leverage point ? Explain your answers.**

```{r}
lm.fit4 <- lm(y ~ x1 + x2)
lm.fit5 <- lm(y ~ x1)
lm.fit6 <- lm(y ~ x2)
summary(lm.fit4)
summary(lm.fit5)
summary(lm.fit6)
```

In the first model, x1 is not statistically significant, but in the second model it is. x2 is statistically significant in both the first and the third model. 

```{r}
par(mfrow=c(2,2))
plot(lm.fit4)
plot(lm.fit5)
plot(lm.fit6)
```

In the first model (x1 & x2) and third model (x2 only), the last point is a high-leverage point. In the second model (x1 only)  the last point is not a high-leverage point. 

```{r}
plot(predict(lm.fit4), rstudent(lm.fit4))
plot(predict(lm.fit5), rstudent(lm.fit5))
plot(predict(lm.fit6), rstudent(lm.fit6))
```

In the first model (x1 & x2) and third model (x2 only), the last point is not an outlier. In the second model (x1 only)  the last point is an outlier, the point is outside |3| value cutoff.
